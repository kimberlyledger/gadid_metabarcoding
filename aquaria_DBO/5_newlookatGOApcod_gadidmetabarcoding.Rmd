---
title: "GOApcod gadid metabarcoding"
author: "Kimberly Ledger"
date: "2023-09-27"
output: html_document
---

analysis of gadid sequences from March 13 2023 sequencing run 
samples from the Gulf of Alaska 
these samples were combined with Amalga samples 
i included two pcr replicates of each sample/extraction
and there are three samples per sampling 'set' of the 2021 pcod survey  

load libraries
```{r, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
```

read in gadid metadata 
```{r}
metadata <- read.csv("/genetics/edna/workdir/gadids/20230313_GOA/20230313_S1_GOA_metadata.csv") %>%
  rename(Sample_ID = SampleID)

#illumina output changed "_" to "-"
metadata$Sample_ID <- gsub("_", "-", metadata$Sample_ID) 
```

read in all primer set's taxonomic identification tables and samples by asv tables
```{r}
taxon <- read.csv("/genetics/edna/workdir/gadids/20230313_GOA/trimmed/filtered/outputs/asv_taxonomy_blastn.csv", row.names = 1)

asv_table <- read.csv("/genetics/edna/workdir/gadids/20230313_GOA/trimmed/filtered/outputs/ASVtable.csv") %>%
  rename(Sample_ID = X)
```


let's start by taking a closer looks at our dataset 
```{r}
## number of ASVs 
sum(grepl("ASV", colnames(asv_table)))  

## number of samples in ASV table 
nrow(asv_table)
```
Before diving into the decontamination steps, let's get a feel for what the data look like. 

### positive controls 

add column to the ASV table that labels the sample type
```{r}
asv_table_with_sample_type <- metadata %>%
  dplyr::select(Sample_ID, sample_type) %>%
  left_join(asv_table, by = "Sample_ID")
```

let's start by visualizing the reads in the positive control samples 
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "positive_control") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads in positive controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

top asvs in positive controls
```{r}
asvs_PC <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "positive_control") %>%
  group_by(ASV) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_PC, 10)
```


### extraction blanks 

let me look into the reads that got into the extraction blanks
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "extraction_blank") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - extraction blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

```{r}
asvs_EC <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "extraction_blank") %>%
  group_by(ASV) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_EC, 10)
```


### pcr blanks 

let me look into the reads that got into the pcr blanks
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "PCR_blank") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr negatives") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

```{r}
asvs_PCRN <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "PCR_blank") %>%
  group_by(ASV) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_PCRN, 10)
```


# 1. Estimate index hopping  -- I CANT DO THIS BECAUSE MY PC WAS A MOCK COMMUNITY SAMPLE... 
subtract the proportion of reads that jumped into the positive control samples from each environmental sample 

identify the maximum proportion of reads for each ASV found in the positive controls
```{r}
prop_asvs_in_positives <- asv_table_with_sample_type %>%
  filter(sample_type == "positive_control") %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop))
```

let's plot the max proportion of reads for each ASV found in the positive controls vs the abundance of reads for that ASV (total) 
```{r}
reads_per_asv <-  asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  group_by(ASV) %>%
  summarise(TotalReadsPerASV = sum(reads))

for_plot <- prop_asvs_in_positives %>%
  left_join(reads_per_asv, by = "ASV") #%>%
  #filter(ASV != "ASV7") %>% ## remove the most abundant asvs from this plot
  #filter(ASV != "ASV10")
  
ggplot(for_plot, aes(x = TotalReadsPerASV, y = max_prop)) + 
  geom_point()
```

subtract the max proportion of tag-jumped reads for each ASV from all samples
```{r}
indexhop_table <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "reads") %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  left_join(prop_asvs_in_positives, by = "ASV") %>%
  mutate(IndexHoppingReads = TotalReadsPerSample*max_prop) %>%
  mutate(reads_IndexHop_removed = reads - IndexHoppingReads) %>%
  mutate(reads_IndexHop_removed = if_else(reads_IndexHop_removed < 0, 0, reads_IndexHop_removed))
head(indexhop_table)
```

```{r}
indexhop_table %>%
  filter(max_prop > 0) %>%
  filter(reads > 0)
```

actually, this step doesn't hurt.. except for removing 242 reads of ASV14 (pcod) from e02052-1 

clean up the table by removing columns no longer needed 
```{r}
asv_table_filter1 <- indexhop_table %>%
  dplyr::select(Sample_ID, sample_type, ASV, reads_IndexHop_removed) %>%
  dplyr::rename(reads = reads_IndexHop_removed)
```

this is a summary of the number of reads removed by ASV and sample_ID
```{r}
decontaminated_1 <- indexhop_table %>%
  dplyr::select(Sample_ID, ASV, IndexHoppingReads) %>%
  pivot_wider(names_from = "ASV", values_from = "IndexHoppingReads")
head(decontaminated_1)
```

and a list of the proportion of reads from ASVs removed 
```{r}
prop_removed_1 <- prop_asvs_in_positives %>%
  arrange(desc(max_prop))
head(prop_removed_1)
```

okay, just to be clear. subtracting the max proportion of ASVs found in the positive controls from the environmental samples and negative controls accounts for tag-jumping. but the subtraction of max proportion of ASVs found in the positive controls from the positive controls just gets rid of the reads in PC completely.  which i think is okay... 


# 2. Account for contaminants in positive and negative controls 

next we will remove ASVs that only occur in controls and not in environmental samples. and then we will subtract the maximum number of reads from ASVs found in the controls from all samples. i will start by working with the extraction negatives, pcr negatives, and positive controls (though there should not really be too many reads left in positives after step 1). 

let's start by taking a look at what reads remain in these controls 
```{r}
asv_table_filter1 %>%
  filter(sample_type != "sample") %>%
  filter(sample_type != "field_blank") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

alrighty, nothing to remove so will skip to step 3. 


# 3. Discard PCR replicates with low numbers of reads 

calculate reads per sample
```{r}
all_reads <- asv_table_filter1 %>%
  group_by(Sample_ID) %>%
  summarize(ReadsPerSample = sum(reads))
```

visualize 
```{r}
all_reads$x_reordered <- reorder(all_reads$Sample_ID, -all_reads$ReadsPerSample)

all_reads %>%
  ggplot(aes(x = x_reordered, y = ReadsPerSample)) + 
  geom_bar(stat = "identity")
```

fit a normal distribution
```{r}
fit <- MASS::fitdistr(all_reads$ReadsPerSample, "normal")

all_reads %>%  
  mutate(prob = pnorm(all_reads$ReadsPerSample, fit$estimate[[1]], fit$estimate[[2]])) -> all_reads
```

identify and remove the outliers
```{r}
low_dist_probability_cutoff <- 0.05
minimum_read_cutoff <- 100

outliers <- all_reads %>% 
  filter(prob < low_dist_probability_cutoff  | ReadsPerSample < minimum_read_cutoff)
outlierIDs <- outliers$Sample_ID
```

which samples are removed because of the 2.5%/1000 reads threshold??
```{r}
replicates_removed_2 <- asv_table_filter1 %>%
  filter(Sample_ID %in% outlierIDs) %>%
  pivot_wider(names_from = "ASV", values_from = "reads")
#head(replicates_removed_2)
```

number of pcr replicates removed
```{r}
nrow(replicates_removed_2)
```

plot them
```{r}
replicates_removed_2 %>%
  pivot_longer(cols = c(3:46), names_to = "ASV", values_to = "count") %>%
ggplot(aes(x=Sample_ID, y=count, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() + 
   labs(
    y = "sequencing reads",
    x = "sample ID",
    title = "samples with low read numbers")  +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

double check that the probability threshold is appropriate. i.e. make sure no replicates with lots of reads (usually >2000 are removed)

filter the data frame 
```{r}
asv_table_filter3 <- asv_table_filter1 %>%
  filter(!Sample_ID %in% outlierIDs)
```

how many environmental samples, control samples did we get rid of here? 
```{r}
removed_2_summary <- replicates_removed_2 %>%
  group_by(sample_type) %>%
  summarize(removed_2 = n())
removed_2_summary
```

did we lose any ASVs during this step? 
```{r}
length(unique(asv_table_filter3$ASV)) 
```

it may be useful to store a list of the samples removed during decontamination step 3
```{r}
samples_removed_2 <- replicates_removed_2 %>%
  filter(sample_type == "sample") %>%
  select(Sample_ID)
#write.csv(samples_removed_2, "gadid_metabarcoding/20230918_decontamination_step3.csv")
```


# 4. Hierarchical Occupancy Modeling 

this is based on work by Ryan Kelly: https://github.com/invertdna/OccupancyModeling_Stan/tree/master

the hierarchical stan model used here: https://github.com/zjgold/gruinard_decon/blob/master/gruinard_decontam_script.R

```{r}
##Stan Model
sink("Stan_SOM_hierarchical_with_occuprob.stan")
cat(
  "data{/////////////////////////////////////////////////////////////////////
    int<lower=1> S;    // number of samples (nrow)
    int<lower=1> Species[S];    // index of species, each of which will have a different value for p11 and p10
    int<lower=1> Nspecies;    // number of species, each of which will have a different value for p11 and p10
    int<lower=1> L[S];   // index of locations or species/site combinations, each of which will have a different value psi
    int<lower=1> Nloc;   // number of locations or species/site combinations, each of which will have a different value psi
    int<lower=1> K[S];   // number of replicates per site (ncol)
    int<lower=0> N[S]; // number of detections among these replicates
    int z[S];   // integer flag to help estimate psi parameter
}

parameters{/////////////////////////////////////////////////////////////////////
    real<lower=0,upper=1> psi[Nloc];  //commonness parameter
    real<lower=0,upper=1> p11[Nspecies]; //true positive detection rate
    real<lower=0,upper=1> p10[Nspecies]; //false positive detection rate
}

transformed parameters{/////////////////////////////////////////////////////////////////////
}

model{/////////////////////////////////////////////////////////////////////
  real p[S];
  
    for (i in 1:S){
			z[i] ~ bernoulli(psi[L[i]]);
			p[i] = z[i]*p11[Species[i]] + (1-z[i])*p10[Species[i]];
			N[i] ~ binomial(K[i], p[i]);
	}; 
  
  //priors
  psi ~ beta(2,2); 
  p11 ~ beta(2,2); 
  p10 ~ beta(1,10);
}

generated quantities{
  real<lower=0,upper=1> Occupancy_prob[S];    //after inferring parameters above, now calculate occupancy probability for each observation. Equation from Lahoz-Monfort et al. 2015
  
  for (i in 1:S){
  Occupancy_prob[i]  = (psi[L[i]]*(p11[Species[i]]^N[i])*(1-p11[Species[i]])^(K[i]-N[i])) 
  / ((psi[L[i]]*(p11[Species[i]]^N[i])*(1-p11[Species[i]])^(K[i]-N[i])) 
  + (((1-psi[L[i]])*(p10[Species[i]]^N[i]))*((1-p10[Species[i]])^(K[i]-N[i])))
  );
  }
 }
  
",
fill=TRUE)
sink()
```


the first step is to format my data for the stan model - i need site information so will start by getting that from the metadata

note: for the gadid aquaria samples, i will treat each tank as a 'site'
```{r}
sites <- metadata %>%
  #filter(MiSeq_run == "A") %>%
  dplyr::select(Sample_ID, shortID, replicate, pcod_set)

rep_table <- asv_table_filter3 %>%
  left_join(sites, by = "Sample_ID") %>%
  filter(pcod_set != 'NA') %>%
  filter(pcod_set != 'unknown') %>% ##remove sample for which we don't have site locations
  arrange(pcod_set)

occu_df <- rep_table %>%
  ungroup() %>%  
  mutate(reads = ifelse(reads > 0, 1, 0)) %>% # change counts to presence/absence
  dplyr::select(ASV, pcod_set, shortID, Sample_ID, reads) %>%
  group_by(ASV, pcod_set, shortID) %>%
  summarise(K = n(),  #count the number of rows for K 
            N = sum(reads)) %>% #sum the detections (now in reads column) for N 
  dplyr::rename(Site = pcod_set,
         BiologicalRep = shortID) %>% ## just renaming so that it matches the naming used in the stan model
  separate(ASV, into = c(NA, "Species"), sep = 3, remove = FALSE)

occu_df$Species <- as.integer(occu_df$Species) #convert ASV to an integer

occu_df <- occu_df %>%
  arrange(Species)
```

**NOTE: i set the shortID as the biological replicate!!!** 

running the occupancy model on this entire data set will take a VERY long time, so now I will reduce the data set down to the patterns of presence (many ASVs/species have identical patterns of presence, aka. pattern of technical reps)

```{r}
pattern.of.replication <- rep_table %>%
  ungroup() %>%  
  mutate(reads = ifelse(reads > 0, 1, 0)) %>% # change counts to presence/absence
 # filter(!grepl("-2-", Sample_ID)) %>%    ## the extraction replicates are messing thing up right now... i need to code this better in the metadata/etc eventually 
  #dplyr::select(location1, biological_replicate, pcr_replicate, ASV, reads) %>%
  dplyr::select(pcod_set, shortID, replicate, ASV, reads) %>%
  pivot_wider(names_from = replicate, values_from = reads) %>%
  rename(A = '1') %>%
  rename(B = '2') %>%
  mutate(ndetections = A + B) %>%
  group_by(pcod_set, ndetections, ASV) %>%
  summarize(tot = sum(!is.na(ndetections)))

pattern.of.presense <- pattern.of.replication %>%
  spread(ndetections, tot, fill = 0) %>%
  unite(repetition.level, '0', '1', '2', sep = '.') %>%
  select(!`<NA>`)

#select a representative 
unique.pattern <- pattern.of.presense %>%
  group_by(repetition.level) %>%
  summarise(ASV = head(ASV,1) ,
            Site = head(pcod_set, 1)) %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F)


#subset my full data frame (occu_df) to just include one representative of each unique detection pattern 
occu_df_subset <- occu_df %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F) %>%
  filter(key %in% unique.pattern$key) 
```


```{r}
temp.df <- occu_df_subset

  #make a new species column to that values are consecutive
  Species <- temp.df$Species
  temp.df$Species_1 <- match(Species, unique(Species))

  #create unique identifier for combinations of site-biologicalrep-ASV; for use in hierarchical modeling
  SDS <- unite(data = temp.df, col = SDS, c("Site", "BiologicalRep", "Species")) %>% pull(SDS)
  temp.df$SiteRepSpecies <- match(SDS, unique(SDS)) #index for unique site-biologicalrep-species combinations
  
  #create unique identifier for combinations of site-ASV; for use in hierarchical modeling
  SS <- unite(data = temp.df, col = SS, c("Site", "Species")) %>% pull(SS)
  temp.df$SiteSpecies <- match(SS, unique(SS)) #index for unique site-species combinations
  
  #####################
  #run Stan model
  #note this will take a while the first time you run a particular model, because it needs to compile from C++
  #####################      
  myHierarchicalModel <- stan(file = "Stan_SOM_hierarchical_with_occuprob.stan", 
                        data = list(
                          S = nrow(temp.df),
                          Species = temp.df$Species_1,
                          Nspecies = length(unique(temp.df$Species_1)),
                          L = temp.df$SiteSpecies,
                          Nloc = length(unique(temp.df$SiteSpecies)),
                          K = temp.df$K,
                          N = temp.df$N,
                          z = ifelse(temp.df$N > 0, 1, 0)
                             ), 
                             chains = 4,   #number of chains
                             iter = 4000   #number of iterations per chain
       )
       
  #myHierarchicalStanResults <- tidy(tibble(as.data.frame(myHierarchicalModel)))
  
  #write_rds(myHierarchicalStanResults, "~/gadid_metabarcoding/gadid_fieldapplication/20230313_MiSeqrun_occupancy_output_20230927.rds")
```


```{r}
myHierarchicalStanResults <- read_rds("~/gadid_metabarcoding/gadid_fieldapplication/20230313_MiSeqrun_occupancy_output_20230927.rds")

  ## occupancy probabilities 
  myHierarchicalStanResults_occu <- myHierarchicalStanResults %>%
    filter(grepl("Occupancy_prob", column)) %>%
    separate(column, into=c("column","SiteRepSpecies"), sep="([\\[\\]])")
  
  myHierarchicalStanResults_occu$SiteRepSpecies <- as.numeric(myHierarchicalStanResults_occu$SiteRepSpecies)
  
  occupancy_prob <- temp.df %>% 
    select(ASV, Species, Site, SiteSpecies, SiteRepSpecies) %>%
    left_join(myHierarchicalStanResults_occu, by = "SiteRepSpecies") %>% 
    group_by(ASV, Site, SiteSpecies) %>%
    summarise(max_Occupancy_prob = max(mean))
  
# join my occupancy probabilities the unique.pattern df and then the pattern of presence... 
occu_with_key <- occupancy_prob %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F) %>%
  left_join(unique.pattern, by = c('key', 'ASV', 'Site')) 

occu_with_key_to_join <- occu_with_key[, c("repetition.level", "max_Occupancy_prob")]

site.asv_occupancy_probs <- pattern.of.presense %>%
  left_join(occu_with_key_to_join)

keepers <- site.asv_occupancy_probs %>%
  filter(max_Occupancy_prob >= 0.7) %>%
  unite(pcod_set, ASV, col = 'filter_id', sep = '.', remove = F)
discard <- site.asv_occupancy_probs %>%
  filter(max_Occupancy_prob < 0.7)
```


```{r}
asv_table_filter4 <- rep_table %>% 
  unite(pcod_set, ASV, col = 'loc.asv', sep = '.', remove = F) %>%
  filter(loc.asv %in% keepers$filter_id)
```

how many unique ASVs are now in the data set? 
```{r}
length(unique(asv_table_filter4$ASV)) 
```
## now join the taxonomy 
join the taxon id's to the ASVs in the decontaminated read dataset 
```{r}
my_asvs <- asv_table_filter4$ASV

decontam_asvs <- taxon  %>%
  filter(ASV %in% my_asvs)
```

```{r}
join <- asv_table_filter4 %>%
  left_join(decontam_asvs, by = "ASV") %>%
  filter(!is.na(taxon)) %>%   ##### remove ASVs without a species ID - check why there's no ID later! 
  group_by(Sample_ID, shortID, replicate, pcod_set, taxon) %>%
  summarize(total_reads = sum(reads))
```

```{r}
my_palette <- brewer.pal(3, "BuPu")
```


plot taxon proportions
```{r}
plot_three <- c("81", "84", "85")

join %>%
  filter(pcod_set %in% plot_three) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(total_reads)) %>%
  mutate(prop = total_reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=taxon)) +
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = my_palette[2:3]) +
  facet_wrap(~pcod_set, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    #axis.text.x = element_blank(),
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "right",
    legend.title = element_blank()) + 
  labs(title = "goa")
```

plot taxon proportions
```{r}
plot_three <- c("75", "76", "78")

join %>%
  filter(pcod_set %in% plot_three) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(total_reads)) %>%
  mutate(prop = total_reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=taxon)) +
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = my_palette[2:3]) +
  facet_wrap(~pcod_set, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    #axis.text.x = element_blank(),
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "right",
    legend.title = element_blank()) + 
  labs(title = "goa")
```

plot taxon proportions
```{r}
plot_three <- c("86", "87", "88")

join %>%
  filter(pcod_set %in% plot_three) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(total_reads)) %>%
  mutate(prop = total_reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=taxon)) +
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = my_palette[2:3]) +
  facet_wrap(~pcod_set, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    #axis.text.x = element_blank(),
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "right",
    legend.title = element_blank()) + 
  labs(title = "goa")
```

plot taxon proportions
```{r}
join %>%
  filter(pcod_set == "92") %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(total_reads)) %>%
  mutate(prop = total_reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=taxon)) +
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = my_palette[2:3]) +
  facet_wrap(~pcod_set, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    #axis.text.x = element_blank(),
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "right",
    legend.title = element_blank()) + 
  labs(title = "goa")
```

okay, to do next. figure out which goa pcod sets i want to include and how i want to show that there were some zero read replicates... recognizing that this sequencing run produced very low read numbers overall... 


maybe plotting these locations on a map would also help. 
```{r}
library(ggOceanMaps)
library(ggspatial)
```


read in sample metadata
```{r}
goa_metadata <- read.csv("/genetics/edna/workdir/GOApcod_2021/GOA2021_metadata_20230630.csv")

#illumina output changed "_" to "-"
goa_metadata$Sample_ID <- gsub("_", "-", goa_metadata$Sample_ID) 
```

get site info
```{r}
sites <- goa_metadata %>%
  filter(location1 %in% join$pcod_set) %>%
  select(location1, longitude, latitude) %>%
  unique() %>%
  filter(!is.na(longitude))

sites$location1 <- as.factor(sites$location1)
```

add sites to map 
```{r}
map_with_sites <- basemap(c(-163, -158, 54.5, 56), bathymetry = TRUE, rotate = TRUE) + 
  ggspatial::geom_spatial_point(data = sites, aes(x = longitude, y = latitude, color = location1), size = 2)
map_with_sites
```





